# MLモデル開発・運用ワークフロー検討会の議事メモ（マスク済）

Date: 2020/02/10
Tags: event, memo
URL: https://mlxse.connpass.com/event/163305/
場所: NTTデータ豊洲センタービル Inforium

# NTTデータ土橋氏による導入(~17:10)

- WGの紹介
- 本討論会の動機・目的
- アジェンダ・進め方

# NTTデータ萩原氏によるたたき台説明 (~17:50)

アジェンダ

1.対象とする範囲

2.用語定義

3.参考にした公開情報

4.全体像

5.個別

- 1.初期学習（試行錯誤）
- 2.初期学習（大規模）
- 3.本番用推論
- 4.継続的学習

# ディスカッションのポイント

- 学習・推論、バッチ・ストリーム（オンライン）の組み合わせ
    - バッチ学習、ストリームデータを使ったオンライン学習、バッチ学習＋ストリームデータの推論、…
    - フィードバックしてモデルを改善していくようなアプローチもある（制御系みたいな話）
- アノテーション
    - モデル品質や運用状況を見てアノテーションしなおすなど。アノテーションの仕組みを自前で持っている企業・組織もある
- データ品質、モデル品質の監視・観測
    - 継続的学習におけるデータの確認は、入力データをそろえるタイミングで実施する、というケースもある
    - システムの監視とは異なる
    - 原因分析が難しい
    - 既存のライブラリやフレームワークは、型のバリデートなどをするだけで、分布などデータ品質に関わるところまでケアしているものはないのではないか
- 問題発生時の対応
    - 業務によって温度感が全く異なる
        - クリティカルな影響あり：適当にさばけない。フェールセーフが必要
        - 可用性重視：最悪ランダムでもよいから値を返す
- アプリケーション開発手法
    - Jupiterノートブックで試行錯誤したものをどうやって本番化するか？
        - そのまま使う人もいる？
            - ただし、研究開発寄りだとそうだが、企業のプロダクショナイズで本当にそうなるのか？
            - 「分析レポート」が最終成果物である場合はノートブックそのままということもある
- 開発体制
    - 本番開発を事業会社に移管してSEがアプリケーション開発、というケースもある。その場合はノートブックの試行錯誤そのままでは辛い。
    - 現場で適用する場合、そこにはデータサイエンティストがいないケースが多い。
    - 会社の中で縦割りになっていて、例えばモデル開発するチーム、アプリケーション開発するチーム、…のように開発体制が分断されているケースはどうするのか？
    - モデル開発が受託なのか、内製なのかで全く議論が異なる
- その他考えないといけないけど、抜けがちな議論
    - 倫理、知財、セキュリティ

Twitter上に、監視はどうするか、各フェーズで引き継ぐもの・引き継がないものは何という意見（D氏

# ディスカッションのメモ

敬称略

- 学習のところはバッチ前提だけど、オンラインは考えなくていいのか？（E社W氏：自社でレコメンドのシステムを取り組んでいる
    - オンライン学習という表現には2種類の意味がありうるので注意して使い分けたい。（A氏
        - １．追加データを既存モデルへ反映させる手法であるオンライン学習という意味　と、
        - ２．ストリーム的に来た1件1件のデータに対して学習を行うという意味
        - がありえるので、"オンライン"がごちゃごちゃになりがち。
    - 発言の意図は２．の意味だった。（W氏
    - モニタリング・モデルのバージョニングなど、ストリームでの学習はすごく難しいという感覚（A氏
        - 公共のユースケース（気象関係）がそれやろうとしたら、外れ値とかが増えて意味のないモデルになってしまった。
        - 学習についてはマイクロバッチとかならありえる。ストリームについて予測はありうるが、モデルの更新はきついのでは。
    - 各個人の入力に対して学習を進めた経験がNTTデータである。1データ1データに意味があったため。（W氏
    - フィードバック制御はオンラインみたいにちょっと小出しにしてもいいかも （N社S氏
    - オンライン学習をしている人は要るか？
        - （注：会場内において、W氏・S氏の２人のみ挙手）
    - マジョリティはまずはバッチベースだと考えており、このワークフロー図はバッチを想定している。（D氏
- アノテーション系の情報を取り込むのはフロー上にあるか？（やしろ、アノテーションなどの基盤
    - アノテって場合によっては本質の場合がある、初期学習の時点でやらないと後で困る場合が多い。
    - 過去の案件ではアノテ用のUIをつくっていた（D氏
        - フローと言われるとどこになるかが悩ましい。
    - データ収集で、付加するような情報も集めることは加味しているつもりはあった。ただしこれをデータ収集と一口で言うと粒度が大きすぎるかもしれない。（H氏
    - 前職ではアノテ部隊がおり、そのためのツールも持っていた。アノテと自分たちのモデルのミスマッチがあると再学習前にアノテーションを取り込む、といった形でプロセス化していた。（HT氏
        - モデルがfixされているという前提だった。場合によっては、ビジネスロジックを加えたり、アルゴリズムを考えたりする場合には当てはまらない可能性がある。
        - ある一定期間ならば、同一のモデルを使いまわしにすることもある。そういったケースであれば当てはまるかもしれない。（D氏
- データセットをいじるケース、アノテをいじるケース、パラメータだけを変えるケース、アルゴリズムごと変えるケースなど、いろいろな場合がある。その中で難しいのは、どのように監視して、その結果を受けてどのように原因分けするか。分類分けできるような定義が重要である。（Y氏
    - インフラ側から見ると、従来の監視ではシステム・業務が変わらない前提で監視するのが普通だった。一方で機械学習だとつなぎこみまで考えると、人間のオペレーションが入りがちになる。
    - 継続的学習でのデータの確認は、インプットの時点で確認することが多い。結果として確認したうえで、妥当性検証に進むことが多い。（K氏
    - TensorFlowやSageMakerなども、データのバリデーションとしてできるのはデータの型・レンジを確認しているだけである。一方で、例えばデータの分布などについては想定通りになってることを前提としており、そこを監視するところまでは至っていないのが現状である。（A氏
    - メトリクスを測定しようにも、後続システムなどを含むケースがあり難しくなる。（D氏
    - データの傾向を見る、加えて既知のデータの精度やビジネス的な精度、予測時の性能（レイテンシなど) を確認するのが必要ではないか。（A氏
- 天気の予測のケースでは、雨量計が壊れていようが予測を出さなければならない。皆様の議論や経験において、予測がおかしいと気づいたときの頻度や間隔が気になる。天気の予測のケースでは、頻度が高いときには10分に一回とかで予測を行う。
    - 自身のケースでは、アルゴリズム側で確信度を出し、その出口でその場しのぎの対応をする、といった運用対処をしていた。（Y氏
        - 結果だけでなく確信度もユーザに見せていた。
    - 広告枠への入札額の低迷などを検知している。オフライン系指標、オンライン系指標で分かれており、そのあたりをしっかり分けてA/Bテストできる基盤をつくろうとしている。（T氏：広告のリアルタイムの入札システムを運用しており、止められないシステムを運用）
    - このワークフローも場合によってはフルセットで要らないかもしれない。レコメンドなどはそういう場合が多いが、逆に車などで人命にかかわるなどの場合は2重にしておくとか （D氏
- 推論結果を待っていると、機会損失が億単位となる。監視が一番の課題と考えており、監視やってる案件がなく、あまりキャッチアップできていない。（I氏：金融系システム
    - レイテンシが短い場合だとミリ秒単位で更新がかかる。レコメンドするというシステムの特性上、何も推論結果を出さないよりも何らかの推論結果を出した方がいいという考えに立っている。（W氏
        - いくつかのフォールバック策を持っており、フォールバックすればするほどだんだん結果の質は悪くなるはず。
        - 最悪時のフォールバックはランダムだが、何も結果を出さないよりはいい。
- 自動運転のケースでは推論結果が不適切だと人命にかかわる。組み込み分野相当でデプロイまでの期間が長くなる。将来的には自動でアップデートすることになると考えているが現状どう考えるか？
    - 監視でアラートがあがれば、ルールベースに切り替えるか、人間系に切り替える。
        - システム系の監視ツールは、用途が違ってはまっていない。現状監視の方法が確立できておらず、システム系の監視ツールを使うべきなのか、BIツールを使うべきなのかがわからない。（D氏
    - 異常を検知したら、例えばランダムにフォールバックするなど。（M氏
- 有賀さんが事前にTwitter上でアンケート（Jupyter Notebookを機械学習のどのフェーズで使うのか。回答数1147vote）を取っていた。このアンケートで興味深かったのはコードの書き直しで、意外とNotebookそのままで使っている例があった。（D氏
    - 35%が最初から最後までJupyterを使っていると回答している。この結果がどこまで実態にそぐうのかはわからない。（A氏
    - 初期学習でPoCをデータサイエンティストがやるとして、大規模開発のフェーズが、大規模か否かではなく、事業会社に移管してSEがアプリケーションを開発するという形になる場合がある。（I氏
        - 初期学習では好き勝手にやっていたり、ドキュメントもない。ここから開発を行うのが大変なことになっている。きちんとガバナンス効かせようとしている。
    - 研究よりの案件・事例が35%に入っているのではないか。（Y氏
    - 商用事例の一種としてNotebookを使い続けるケースもある。例えば会計系。データを集めるところまでで、分析して結果みるのは会計士の場合など。この場合はNotebookだけを納品するケースもある。（W氏
    - 各工程に入るステークホルダーはどのように考えているか。（I氏
        - ビジネスや現場寄りの人は考えきれていないのが正直なところ。（H氏
        - 現場でやる場合ってデータサイエンティストが抜けている場合が多い、運用まで残ってもらえないなら、どうやって担保するのかが課題となる。
    - モデルの再構築はお客様の責任で実施することとして責任分界点を定めるケースもあり、その場合だとお客様も巻き込んだワークフローが必要となる。（I氏
        - ビジネスオーナーが絡んでくる。（D氏
        - また、会社間に限らず、会社内の組織の縦割りでどのように連携するかを考えないといけないケースもある。例えば、ある組織はモデルを作るところしかやりたくないなど。（D氏
    - モデルの改善が必要だが、その必要性を説得するコストが高すぎて、終わってしまったことがあった。（A氏
        - たとえば機械であれば油を刺さないといけないといったメンテナンスコストを理解してくれる。しかし機械学習に関してのメンテナンスコストは現状中々わかってもらいにくい。
    - モデルの更新も、自動化できるレベルなら稼働範囲内になる。一方で、データサイエンティストが必要で、新たに分析したりチューニングしなければならないレベルだと、話が変わってくる。（I氏
        - 初期投資は0からの改善なので費用対効果がわかりやすく説得しやすい。しかし、モデルの再構築は人的コストは初期投資時とあまり変わらない割には初期投資時と比べた時の改善度合いが小さくなってしまうことから、説得がしにくい。（A氏
        - 機械学習で顧客価値を出せるシステムはとても限られている。（I氏
        - 機械学習の経年による劣化具合を端的に言うのが難しい。システムだけの話じゃなくて、どうやって価値に対するコンセンサスを得るかや、機械学習によって得られた結果がビジネス価値としてどのような数字として得られるか、その価値を可視化するかなどをセットで考えなければならない。（D氏
        - 最初のKPI設定がミソとなる。ここでうまく顧客と握れるかが重要で、それを握れないままだと、やってはいけないプロジェクトが進んでしまう。（I氏
            - どこで撤退するのかの判断ポイントも入れることが重要。（D氏
        - 今まで機械学習に取り組んできた中で、それらの取組を結果的に捨て続けた歴史もある。最初はよい結果が得られても、結果が劣化してしまうと誰もモデルを活用しなくなってしまう。システムとして機械学習が動いていようとも結局人が分析することとなり、業務レベルが下がる。（S氏
        - 機械学習はその結果が効果的でありづづけるかを保証するのが難しい。こういった、予測不可能性みたいなところをどう握るかが問題となる。（D氏
            - 機械学習で得られる結果についてのKPIを定めてコミットするのが難しい。（Y氏
                - 実地ではなく、ローカルで試した結果だけからKPIを定めるのは怖い。
            - 初期学習の中で試行錯誤を複数回行い、改めてビジネス上の目標を決める、といったフローがあったほうがいい（Y氏、D氏
        - このような話しって、A/Bテストやっても起きている（D氏
        - データのドリフトが確認できなかったということが原因（S氏
            - 自分たちの取組として、どれだけドリフトしてるかはわからないけど、監視はしている。（I氏
            - レコメンドだとデータが変わる前提で、自動で最適化するようにしている。（W氏
            - データはいずれにせよドリフトする。ドリフトする前提として人間の手を介在できないかどうかは考えるようにしている。例えば予測のスコアのトップ5だけを選ぶとか、業務側で逃げを作るようにしている。（YM氏
                - 同様（I氏
- ビジネス課題設定でデータを見たりEDA的なことをしている。スコープに入らないか？（TT氏
    - 対象とする案件にもよると思うが、考えないといけないと思っている。（D氏
        - 例えば、データが大規模になってしまう場合だと、最初の収集のために相応の規模のシステム基盤を用意しなければならない場合も多い。
    - TreasureDataの場合はそのプラットフォーム上でデータを見たりEDAする。顧客が個人情報を気にする、セキュアにやりたい人向けの環境が必要となる。（A氏
    - データをローカルにダウンロードできるようにするとデータのアクセス制御として監査の問題となる。現在、自社のサーバにJupyterやVSCodeのサーバ版を導入することで、ローカルで直接データをアクセスしないようにしている。（TH氏
    - 現在のワークフロー上には、監査を受けるというフローやセキュリティチェックのフローが入っていない。またAI倫理の観点でリリースを判定するタイミングは悩ましい。（D氏
        - また、機械学習にはデータを用いる以上、その知的財産権についてのクリアランスも重要。（I氏
        - 扱うデータに関しては、各フェーズにおいて元データと成果物を紐づけることでトレーサビリティを確保することが必要。例えばGDPRで求められる個人情報のオプトアウトのためには、その個人情報が何に使われているかを追跡できる必要がある。（H氏
- 実業務におけるワークフローは、受託開発のプロジェクトなのか、内製するプロジェクトなのかで必要な要素が大きく変わってくる。（Y氏
    - 受託開発の場合だと顧客との折衝や合意形成が多く必要になる。そうした場合は、現状のワークフロー図に加えて、さらに細かくフローを定義する必要があるかもしれない。

# NTTデータ土橋氏によるまとめ

- 何回か議論を繰り返して、論文みたいな形で公開して社会に貢献する形に
- slackやtwitterでも連携しましょう

# Treasure Data有賀氏によるまとめ

- 4月以降、こんなシステムを組んで、こんな学びがありましたみたいなポスターセッションを予定
- 新規性がどうとかはあまり気にしない
- インタラクティブに議論する場を設けるつもり
