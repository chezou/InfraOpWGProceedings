# MLSE夏合宿2020 企画セッション討論会メモ

Date: 2020/07/04
Tags: event, memo
URL: https://mlxse.connpass.com/event/175970/
場所: オンライン開催

## 本メモについて

[MLSE夏合宿2020] にて、 [本番適用のためのインフラと運用WG] が企画した討論会の議事メモである。
討論会の中で活発に議論されたことの多くを記録したが、公開できない情報については一部削除等してあるためご了承願いたい。

[MLSE夏合宿2020]: https://mlxse.connpass.com/event/175970/
[本番適用のためのインフラと運用WG]: https://sites.google.com/view/sig-mlse/wg#h.p_Of4vDn0AZIz-

## 討論会について

[MLSE夏合宿2020] における企画セッションのひとつとして開催した。
[本番適用のためのインフラと運用WG] 幹事の有賀（Arm Treasure Data）、土橋（NTTデータ）がファシリテータとなり、
多様な業界・組織から集まった19名で活発な議論を実施した。

議題は以下の2個である。

- トピック1
    - 機械学習における監視・観測とアーキテクチャ例
- トピック2
    - 責任分解とシステム

なお、2個めの議題についてはもうひとつの候補「アノテーション手法とアーキテクチャ」も含めて、当日参加者意見を募って選択した。

## トピック1 機械学習における監視・観測とアーキテクチャ例

- 機械学習は結果が100%保証されるものでないというのは共通認識だが、ロボット系などでは「正しい出力ができなかった場合にシステムを停止する動作」を担保する必要がある。そのあたりの連携についても観点に含まれるか。→Yes
- 自動運転で判断を迷ったとき、ドライバーの人命が大事なので安全な状態に移動する（停止する）というのが求められる
    - **安全弁は2重3重の機構が必要になる★**
    - MLのシステムは入力データによっては落ちることがあるので皆さんどうしていますか？
    - 例えば、アルゴリズム部分が落ちても、何かしら値を返すようにしておくなど工夫が必要

- たとえば人命がかかわらない領域(アドテクなど)の監視観測はどのような例があるか？（人命がかからないからと言ってクリティカルではない、というわけではないが）
    - 広告システムの例。レコメンドなど。精度が悪くなった場合にどういう被害があるかという話がある。ダイナミック広告（商品を並べて表示）の領域であれば最悪ケースではランキング順の表示。RTB、枠を確保するシステムで極端精度に悪いモデルがデプロイされると広告効果が上がらないということが起こる。そうすると次の受注を逃すことにもつながる。そのため、いっそのこと入札しないという方式をとることもある。
    - 入札は経営のKPIに直結することが多い。クリティカルな問題だったらSlackに通知、飛び起きて対応・・・というケースもある。
    - ログベースの場合はElasticsearchに投入、アラートのルールを設定する、など現状の対応。
    - **ML由来のメトリクスだけでなく、業務上のメトリクスとも突合するようなしくみが必要だということ。★**
        - エラーが発生しているという現象はどう定義されているか？エラーが発生しているのがわかってアラートが飛ばせるのであれば、「想定していない状態」がキャッチできているということになる。どのようなルールか？
            - CTRが高騰している、低迷している、等。異常行動をルールベース、通常の監視系で弾くことができる。通常のクリックレートを常に監視していて、異常な振る舞いはディレクターも（人手で）確認。ラグもあるが、なにをエラーとするかは最終的な広告効果、CTR予測の値を中心にルールベースで弾く。
        - 広告値の「予測のCTR」「実測のCTR」どちらを用いるか？
            - 両方。ES、Kibanaに飛ばし、目視でも確認している。
- コロナで消費者の勾配行動は変わっているが。プリコロナのモデルが役に立たないともいわれる。コロナ前後で変わったか？

    [Machine-learning models trained on pre-COVID data are now completely out of whack, says Gartner](https://www.theregister.com/2020/06/23/covid19_pandemic_means_data_from/)

    https://www.microad.co.jp/news/detail/1257/

    - **機械学習エンジニアとしては入力データもモデルも毎日更新しているので、現状は対応できているという認識で動いている。毎日モデルを更新するのが大切。★**

- 気象系ドメインでの特殊な監視の事例は？
    - 流体力学など普通の気象モデルがある。入力データの気象観測データをチェックする体制、出力をチェックする部門がある。（人間がチェックする仕組みがある）準リアルタイムでもあり、週次、月次、年次のチェックもある。KPIという言い方はしないがKPIにあたる。
        - **補足：これまでに独自の手法を用いてきた領域では、それらと合わせて機械学習を用いる。その際、監視・観測上はこれまでの手法で用いてきたメトリクスとセットで考え、判断できるようにする工夫が必要、という議論★**
    - （架空の話）ビールの売れ行きと組み合わせるときはそのデータも正常性を確認する
        - 日次でモデルを常時アップデートする体制だったので、コロナの対応もできた
    - 正常性の監視について、人手は介在している。天気予報なども見つつ、「大雨の日はビールの予測が外れたね」といった振り返りも実施している。
        - **数年前の山梨の大雪の例でいうと、いくつかの機械の予測結果では、大雪を予報しているものもあった。しかし、「本当にそんなにふるだろうか」として、雪の予報の程度を少し抑えた。人間のチェックも絶対ではない★**
        - ↑一晩で1m雪が降って人が出られない規模の大雪だった
        - **閾値をゆるくして、人間に判断を委ねている。厳しい閾値にすると、そもそも人間に結果が届かないので。★**
- 上記で気象の話をしたが、化学工業などでも同様の事例があるのではないか。モデルの予測をうのみにしていいのか、など。
    - まさにそういうケースがある。 **モデルを現場の担当者が理解できない場合は、適用できない。解釈できるものをつかう、ブラックボックスすぎる手法は避けるようにする、などの対応をとっている。★**
    - KPIがしっかり決まるものについては良いが、そうでないものもあると思っている。例えば品質保証の例だと、普段はOK/NGの閾値はお客様と合意する必要がある。 **MLが出力した閾値についてもお客様と合意が必要。再学習の度に新たな閾値をお客様と会議/合意するのは高コストであり、課題である。現状は、PJごと/モデルごとにケースバイケースで対応している。★**
        - モデルの閾値、新しいものを適用するために毎回会議を開くというトピックでは、「モデルのガバナンスをどうやって保つか？」という議論がある。 **MLflowでは [モデルレジストリ](https://www.mlflow.org/docs/latest/model-registry.html) という機能があり、登録されたモデルが管理できる。承認プロセスがWebアプリケーションとして組み込まれている。そのような機能がOSSとして出ていると、それがどのデータから、どのアルゴリズムから来ているか確かめたい、、という話も一般のユースケースとして議論になっていると思われる。★**
        - **事前に「こうなったらモデルを再作成する（工数を確保する）」というルールを握っておく必要がある★**
            - それをしないとモデルが腐った時にどうしようもならないケースがある。
            - 握るタイミングはいつか？
                - 早めの方が良い。開発や運用のサイクルが別れている場合は、運用にわたす前に入れておけると理想的。SIerなどは開発などフェーズが別れていて課題がありそうだがどうか？
                    - **後半の議論になるかも。試行錯誤する人、運用する人、などロールが別れている。指標となるものを共通言語として渡していかないと、伝言ゲームになる。どのタイミングで何を考えるかという話はプロセス図として内部で作成・利用 ★**

- すぐにコンバージョンにつながらないような指標は過去にあった。が、それはそれで難しく、それが正しいとは限らないため、 **リスク込みで監視する★** 、という判断もあった。
- 予測した結果が正しかったかが後からわかる、というのが **遅行指標★** でよいか
    - 予測が評価されるタイミングと、人がチェックするタイミングがずれているもの
- 早めに「正常に動いてるか」が確認したい、というのは一つのモチベーション
- 工場で仕事するときは、健康状態の防止はMUSTだが、ライン設計時には指標がない。1年後に実際のインシデント数と予測を照らし合わせるようなケースが遅行指標か？
    - Yes
- インフラエンジニアの一般的な監視では、監視対象のメトリクスはある程度パターン化されている。 **機械学習でもドメインにもよるものの一定のパターン化はできるかもしれない★** 。
- **質問： 予測に基づいて介入が発生するような業務において、予測モデルの精度の監視を、どのように行っているでしょうか。例えば故障予測をした場合に、故障が起きる前に交換や修理をするために、本当に故障が起きたはずかどうかのGround Truthが得られないと思うのですが。★**
    - 実際に予知保全では困ります。工場のラインを止めるとかの話。解がなくて困ってます。
    - 「NGをお客様に出さない」とすると、本当の不具合なのか過学習なのかなどもわからなくなってしまう。不具合のシグナルがある程度わかってから適用する、などアイデアが必要かもしれない。大きな目的に対する細分化したシグナルなど先行指標があってもいいかもしれない
    - 経営者から「どのくらいの効果があったのか」と問われるケースも同様で、困る。「導入効果」をどのように示すか。
    - 導入前にモニタリングシステムを組み込み、導入前の状況を把握して比較する必要がある。
    - **ML単体の評価だけでなく、「故障発生時のトータル(対応)コスト」など、ビジネス的な評価も実施することが重要。そういった話を含めれば、上層部や経営者にも説明しやすいはず。★**
    - この問題についてある専門家の意見としては **「必ずエキスパートの人に確認する」★**とのこと。（ **エキスパートが1週間後に故障するというのを、Ground Truthとする、という判断★** ）
- メトリクスは、後で使うかどうかは置いといて一通り取得する仕組みを作るのが良いのだろうか。ML指標もビジネス指標も。「このように作りこんでます/組み込んでます」という実例はあるか。
    - **元々あったZabbixなどに組み込んではいる。そうすると見えてくる指標が細かいインフラ寄りになってしまうこともある。インフラ見るには良いけど合わないこともあった。★よりサービスの監視をすべきとの課題感となった。ディレクターを巻き込んで、「サービスとしてクリティカルなところ」について定義化し、それをフックする為に必要なモニタリングをどうするか、そのメトリックをどう定義するかといった流れで１つ１つ整理している。そうすると各メンバーが主体的にメトリクスについて取り組んだ、という感覚もあり、地道に泥臭く関係者を巻き込むというのが重要。★**
    - 1箇所にまとめている？
        - 分けている。 **1箇所にまとめて多くすると、ノイズも多く入り、自分の関心事から離れた情報が多くなってしまう。★**
    - **Viewはロールごとだが、数値として横串通して見れるように仕組みをつくるのも大切かもしれない。★**

## トピック2 責任分解とシステム

- 「ドメイン知識のある現場のエンジニアへのフィードバックをどう設計するか」について
    - 機械学習やソフトウェアに詳しくないメンバとどうコミュニケーションして全体最適とするか、という意図が含まれる
- 各ロールが別れているか、ある程度単独チームで一気通貫でやっているのか
    - **みんながフルスタックでやっている。別れ方という言い方はあまりない。★**
        - ライフサイクルでいうとどんな例？
            - 開発・運用で分けるのではない。 **[デモグラ](https://markezine.jp/word/detail/%E3%83%87%E3%83%A2%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%EF%BC%88%E3%83%87%E3%83%A2%E3%82%B0%E3%83%A9%E3%83%95%E3%82%A3%E3%83%83%E3%82%AF%E5%B1%9E%E6%80%A7%EF%BC%89) 、ダイナミック分割、CTR予測などトピックごとに分ける。問題なのは、辞めてしまった場合にどう引き継ぐか。こう分けると試行錯誤〜プロダクションコードまでできるが、リスクもある。★**
            - 引き継ぐトピックによってアーキテクチャも様々。共通化できる部分があるとすると、 **モデルの学習、MLOps的なところは共通化できるかも★**
            - **違っている部分は出てくるが、共通部品は共通部品としてアーキテクチャ見直している。★**
- 小規模チームの例
    - **小規模チームの場合、データサイエンティストロールとデプロイを兼務しているので、どうデプロイするかを考慮している。★**
    - 最終的にプロダクトを運用し続けることは少ない。小規模チームの悩みとして、Webサービスなどのインテグレーションを持っていない、イベントトリガが発生しないので、ここを作り込まずに如何に叩くかがポイント。 **iPaaSを組み込むことで、Slackから叩くという組み合わせで、気軽にAPIを叩くことができる。★**
    - 工場がたくさんある。工場ごとのシステムが複数あるのでどう連携するのかがポイント。（自分で組むのか、既存システムの監視にのせるか・・・など）
        - いわゆる情シスに求められる運用のスキルと機械学習基盤に求められる運用の質が違うと想像しているが、どうしているのか？何起因でアラートを挙げて、どのように対応するのか、など。
            - **そこが問題意識でギャップを感じているところ。会社の風土として変わる必要があるという課題感で、全社的に内部の人材の教育を行っている。トレーニング中。そういった観点の一貫として、運用を既存のISに移管するという観点もある。★**
        - 事例として、「会社単位でマシンラーニングの教育コンテンツを持っていて、営業含め全員受講必須」、というケースもあった。効果がでるのに時間はかかるが、年単位でやっていると会話ができるようになる、というケースもあった。(↓かな？？)
        - https://jp.techcrunch.com/2017/05/25/20170524airbnb-is-running-its-own-internal-university-to-teach-data-science/
- 大規模チームの例
    - 事例1
        - 中身がわかるモデルを要求されることがあり、 **責任分界点は「モデルを採用するかどうか」。モデルの中身やシステムについては丁寧に説明する必要がある。★**
            - **↑運用担当者にも、モデルの中身などをある程度理解してもらい、採用の意思決定をしてもらう必要があるため。★**
        - システムの運用性などはデータサイエンティストにはわからない。
        - **どういうデータをどういうモデルに食わせてどのような結果が出たか、を説明するところまでが研究開発の責任/担当。実際に開発して運用に乗っけていくのは、商用運用側の担当。★**
        - ↑研究開発と商用運用で、予算として分離されている必要があるため。
            - 研究開発の結果を本当に正しく商用運用のせられているか、は範囲外。 **想定と違うような運用されてしまうと困る反面、予算の仕切り的に、そこまで担当はできない、というジレンマ。★**
            - 上記解消のために、R&Dから商用運用へ人が動くことはあるか？
                - **先行開発の対象となるサービス部門が、後続の商用開発の部門に異動することはあり得るし、その方が効率がよいと考えられる。★**
    - 事例2
        - **運用入ったときにモデルの再訓練どうするか。GPUクラスタで何十時間回すような訓練なので、顧客が簡単に再学習はできない。訓練のプロセスやモデルは開発者側で持ち、APIとして提供する。★**
        - Googleの音声認識のAPIなどもそういう提供形態
        - 一般の企業の研究部門で提供すると、GCPやAzureなどのAPI提供とどう差別かするか？
        - 体制の問題もある。企業によってはAPIを提供するシステムの運用体制を組むのが難しそう。
            - **少なくともAPIの運用体制が整っている必要がある。インフラ屋も内包するようなマシンラーニング部隊など。★**

    - 事例3
        - 別の会社と業務提携でいっしょに研究している。 **複数ステークホルダがいる環境。同じように研究に入って、ノウハウをいっしょに残す、という座組。★**
            - アプリや仕組みはそれぞれの会社が自由に使っていいよ、というルールをつくる。AIの機能面での責任分界はしていない。ステークホルダ単位で役割分担もせず、なぜそのノウハウを獲得するか、というモチベーションを持ってやっている。その観点は責任分界点を消す点においては役に立っている。
            - 先程まではブラックボックスを隠蔽するアプローチで、本件はフルオープンなアプローチと感じた。
    - FYI 知財的な話
        - http://ibisml.org/ibis2019/files/2019/12/slide_kakinuma.pdf

    - 事例4
        - **システム、プロセス等の標準化をして展開しようとしたが、受け入れられなかった。★**
            - どのような課題になった？
                - **すでに個別の部門ごとに基盤やパイプラインがあった。★**
        - 個別の部門で作成しているパイプラインと、標準化したものとで大きな違いはあったのか。
            - ほとんどなかった。しかし、**標準化するメリットが受け入れられなかった。★**
            - そういうパイプラインを独自に作成できる技術者がいる環境では、標準化に持っていくのは難しい。
        - **インセンティブ設計も重要★**
            - パイプラインを提案したときに**「運用をこちらでもつ」という話をしたときに、個別の要望をすべて受け入れるのは難しかった。結局既存のマネージドの再開発のようになりがち。★**
            - 今だとk8s、kubeflow等のツールが充実しているのでやりやすいかもしれない。
        - 個別で作れるとのことだが、作っている人同士でナレッジシェアしたらいい感じで作れそう。そういう動きは？
            - 実施している。 **そのハブのような部署として、社内の教育なども合わせて実施している。★**
            - **もの作っているところが責任を請け負う、というシンプルなしくみだった。★**
                - **開発前に責任分界点を全部決める。ML製品などは運用が大変なので、その面倒をみる全社組織もある。★**
                - MLナレッジは特殊部分も多いので、全社共通で担保したいという話は理解できる。
- R&D含めて、試行錯誤の上で本番適用していると思う。その際の工夫点や活用技術はあるか。例えば、Metaflow↓など。
    - https://metaflow.org/
        - Netflixのフレームワーク。pythonをメインで使うメンバが多いのでフレームワークを自主開発した。pythonなのでアノテーションを付与するようなプログラミングパラダイム。
    - 事例1
        - **本番に適用する前の話では個人個人で研究。ABテストなど、本番に適用しないとわからない話もある。デモグラ推定など。プロジェクトごとにABテストが掛け算で増える。★** そういうところはMLflowのように機械学習の共通基盤を作っているところ。
        - **具体的にはgithubで全部管理している。リポジトリにCTR、CVRを格納。リポジトリをもとにJenkinsやAnsibleが走り、RTBの本番サーバ同等のプログラムが走る…という仕組み。★**
        - ABテストあたりが、本番適用に向けてはネックになると考える。最初の研究時点は少人数。
        - **オペレーションが複雑になるので、CI CDなどは自動化必須。★**
    - 事例2
        - Azure functionにデプロイしたものをiPaaSで叩く。github actionsも併用。
        - デプロイしたモデルをどう使うかの組み込みに苦労するケースが多いが、iPaaSが便利。
    - 事例3
        - 週に数回モデルを更新している。 **性能劣化があった場合は報告が来るが、その原因特定に時間がかかる。★** 例えば、知らないうちに評価データが更新されていた、といったケースがあった。
        - ↑のように、 **評価に使うデータセットが追えていなかったので、モデル管理、リネージの管理をしようとしている。★**
        - **MLflowのバージョンがガンガン上がって学習バッチが止まると困る問題★**

## クロージング

- ナイトセッションでもMLOPSネタは盛り上がっていたし、各企業で様々な取り組みを実施していると思っている。参考になる部分も多々あったと思う。
- 自己紹介以外でも、「この記載は消してほしい」という部分があれば、直接編集orDisordでコメントいただきたい。
- 秋(9月)に機械学習基盤の事例共有を行うオンラインカンファレンスを企画中（有賀）
